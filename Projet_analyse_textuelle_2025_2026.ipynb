{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwhvVnIDgkStmCgpQMb3ds",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khadijaissoufou/NLP-Job-Market-Analysis/blob/main/Projet_analyse_textuelle_2025_2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROJET TEXTOMETRIE**"
      ],
      "metadata": {
        "id": "r-9c6VQ933mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Choisir le fichier depuis le pc\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "sE1jDXp6n5s6",
        "outputId": "546811c4-44b6-4e7e-d5b0-00f60fd34990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c07825c-08f0-4b71-9dbb-f4d0bae40f85\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1c07825c-08f0-4b71-9dbb-f4d0bae40f85\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Emploidata.txt to Emploidata.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "!pip install stanza -q\n",
        "import re\n",
        "import stanza  # tout en minuscules\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "# 1. Initialisation de Stanza\n",
        "stanza.download('fr')  # à faire une seule fois\n",
        "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma', tokenize_pretokenized=False)\n",
        "\n",
        "# 2. Lecture du fichier\n",
        "with open(\"Emploidata.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texte_brut = f.read()\n",
        "\n",
        "# 3. Nettoyage basique\n",
        "texte_nettoye = re.sub(r\"[,:;'\\\"()\\n]+\", \" \", texte_brut)  # suppression ponctuation et retours à la ligne\n",
        "# Nettoyage du séparateur ****\n",
        "texte_nettoye = re.sub(r\"\\s*\\*{4,}\\s*\", \"****\", texte_nettoye)\n",
        "\n",
        "# 4. Découpage en offres\n",
        "offres = [o.strip() for o in texte_nettoye.split(\"****\") if len(o.strip()) > 50]  # filtre les blocs trop courts\n",
        "print(f\"Nombre d'offres valides : {len(offres)}\")\n",
        "\n",
        "# 6. EXTRACTION TITRE / CORPS\n",
        "idtex = []         # Liste des titres (1er mot de chaque offre)\n",
        "texte_corps = []   # Liste du corps de l'offre (tout sauf le titre)\n",
        "\n",
        "for offre in offres:\n",
        "    mots = offre.split()\n",
        "    titre = mots[0]                # Premier mot = titre\n",
        "    corps = \" \".join(mots[1:])    # Reste = corps\n",
        "    idtex.append(titre)\n",
        "    texte_corps.append(corps)\n",
        "\n",
        "# 7. PASSAGE DANS STANZA ET EXTRACTION DES INFORMATIONS\n",
        "# ON TRAITE TOUTES LES OFFRES\n",
        "all_mots = []\n",
        "all_lemmes = []\n",
        "all_pos = []\n",
        "all_feats = []\n",
        "all_caract = []\n",
        "\n",
        "for offre in texte_corps:\n",
        "    doc = nlp(offre)\n",
        "    for sentence in doc.sentences:\n",
        "        for word in sentence.words:\n",
        "            all_mots.append(word.text)                 # mot original\n",
        "            all_lemmes.append(word.lemma)              # lemme\n",
        "            all_pos.append(word.upos)                  # catégorie grammaticale\n",
        "            all_feats.append(word.feats)               # traits grammaticaux\n",
        "            all_caract.append([word.text, word.lemma, word.upos, word.feats])  # tableau complet\n",
        "\n",
        "\n",
        "# 8/IMPORTS\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# CHARGER LA LISTE DE STOPWORDS DEPUIS GITHUB\n",
        "url = 'https://raw.githubusercontent.com/gillesbastin/french_stopwords/main/french_stopwords.csv'\n",
        "stopwords_df = pd.read_csv(url, sep=';')\n",
        "stop_words_liste = stopwords_df[\"token\"].tolist()\n",
        "\n",
        "# LISTES DE MOTS À EXCLURE\n",
        "auxiliaires = {'être', 'avoir', 'faire', 'pouvoir', 'devoir', 'aller', 'vouloir', 'hésiter', 'venir', 'partir', 'rejoindre'}\n",
        "noms_generiques = {'entreprise', 'action', 'propos', 'offre', 'mission', 'stage'}\n",
        "adjs_generiques = {'nouveau', 'grand', 'principal', 'possible', 'divers', 'fort'}\n",
        "\n",
        "# FILTRAGE ET NORMALISATION\n",
        "caract_b_rsw = []\n",
        "for (mot, lemme, pos, feats) in all_caract:\n",
        "    lemme_lower = lemme.lower()\n",
        "    # On retire stopwords et chiffres\n",
        "    if lemme_lower not in stop_words_liste and not re.search(r'\\d', lemme_lower):\n",
        "        caract_b_rsw.append((mot.lower(), lemme_lower, pos))  # tout en minuscules\n",
        "\n",
        "# EXTRACTION PAR CATÉGORIE AVEC FILTRAGE\n",
        "verbs = [lemme for (mot, lemme, pos) in caract_b_rsw if pos == 'VERB' and lemme not in auxiliaires]\n",
        "adjs  = [lemme for (mot, lemme, pos) in caract_b_rsw if pos == 'ADJ' and lemme not in adjs_generiques]\n",
        "nouns = [lemme for (mot, lemme, pos) in caract_b_rsw if pos == 'NOUN' and lemme not in noms_generiques]\n",
        "\n",
        "# TOP MOTS\n",
        "top_verbs = Counter(verbs).most_common(20)\n",
        "top_adjs  = Counter(adjs).most_common(20)\n",
        "top_nouns = Counter(nouns).most_common(20)\n",
        "\n",
        "print(\"Top 20 VERBES :\", top_verbs)\n",
        "print(\"Top 20 ADJECTIFS :\", top_adjs)\n",
        "print(\"Top 20 NOMS :\", top_nouns)\n",
        "\n",
        "# 9/ LISTE HARD SKILLS\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# --- Liste initiale et additionnelle de hard skills ---\n",
        "hard_skills = [\"Python\", \"R\", \"SQL\", \"NoSQL\", \"MongoDB\", \"PostgreSQL\", \"MySQL\", \"Oracle\",\n",
        "               \"Excel\", \"VBA\", \"Tableau\", \"PowerBI\", \"QlikView\", \"Looker\", \"Google Data Studio\",\n",
        "               \"SAS\", \"SPSS\", \"Hadoop\", \"Spark\", \"PySpark\", \"TensorFlow\", \"Keras\", \"Scikit-learn\",\n",
        "               \"PyTorch\", \"Numpy\", \"Pandas\", \"Matplotlib\", \"Seaborn\", \"Plotly\", \"Dash\", \"D3.js\",\n",
        "               \"API\", \"Cloud\", \"Git\", \"GitHub\", \"GitLab\", \"Docker\", \"Kubernetes\", \"Airflow\",\n",
        "               \"Jenkins\", \"ETL\", \"Talend\", \"Informatica\", \"Data Warehousing\", \"Data Modeling\",\n",
        "               \"BigQuery\", \"Redshift\", \"Snowflake\", \"Azure\", \"AWS\", \"GCP\", \"Machine Learning\",\n",
        "               \"Deep Learning\", \"NLP\", \"Computer Vision\", \"Streamlit\", \"Power Query\", \"Power Pivot\",\n",
        "               \"Tableau Prep\", \"dbt\", \"Jupyter\", \"Colab\", \"RStudio\", \"KNIME\", \"Alteryx\", \"Excel Modeling\",\n",
        "               \"HBase\", \"Cassandra\", \"Kafka\", \"Flink\", \"NiFi\", \"ElasticSearch\", \"Databricks\", \"Hive\",\n",
        "               \"Pig\", \"Spark SQL\", \"Spark Streaming\", \"MLlib\", \"OpenCV\", \"NLTK\", \"spaCy\", \"Gensim\",\n",
        "               \"FastAPI\", \"Flask\", \"Django\", \"Neo4j\", \"Redis\", \"RabbitMQ\", \"Airbyte\", \"Prefect\", \"Luigi\"]\n",
        "\n",
        "nouveaux_terms = [\n",
        "    \"python\", \"sql\", \"excel\", \"power bi\", \"tableau\", \"sas\", \"r\", \"java\", \"spark\",\n",
        "    \"hadoop\", \"airflow\", \"git\", \"linux\", \"machine learning\", \"deep learning\",\n",
        "    \"data visualization\", \"etl\", \"cloud\", \"azure\", \"aws\", \"gcp\", \"nosql\",\n",
        "    \"pandas\", \"numpy\", \"scikit-learn\", \"tensorflow\", \"pytorch\", \"kafka\", \"powerpoint\",\n",
        "    \"analyse de données\", \"statistiques\", \"data engineering\", \"big data\"\n",
        "]\n",
        "\n",
        "# --- Fusion et nettoyage (tout en minuscules + suppression des doublons) ---\n",
        "hard_skills_fusion = sorted(set([h.lower() for h in hard_skills] + [n.lower() for n in nouveaux_terms]))\n",
        "print(f\"Nombre total de hard skills fusionnées : {len(hard_skills_fusion)}\\n\")\n",
        "\n",
        "# --- Lecture et nettoyage du texte ---\n",
        "with open(\"Emploidata.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texte = f.read().lower()\n",
        "\n",
        "# --- Comptage des occurrences exactes ---\n",
        "competences_trouvees = []\n",
        "for comp in hard_skills_fusion:\n",
        "    occur = len(re.findall(rf\"\\b{re.escape(comp)}\\b\", texte))\n",
        "    if occur > 0:\n",
        "        competences_trouvees.append((comp, occur))\n",
        "\n",
        "# --- Préparer le dictionnaire pour le WordCloud ---\n",
        "competences_dict = dict(competences_trouvees)\n",
        "\n",
        "# 10--- Génération du nuage ---\n",
        "plt.figure(figsize=(10,6))\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color=\"white\",\n",
        "                      colormap=\"viridis\", max_words=50).generate_from_frequencies(competences_dict)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Compétences techniques les plus citées dans les offres de stage (WordCloud)\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# --- Affichage top 15 compétences ---\n",
        "df_comp = pd.DataFrame(competences_trouvees, columns=[\"Compétence\", \"Occurrences\"]).sort_values(by=\"Occurrences\", ascending=False)\n",
        "print(\"\\nTop 15 compétences citées :\")\n",
        "print(df_comp.head(15))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gf8su9MUnNqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DESCRIPTION DE LA BDD"
      ],
      "metadata": {
        "id": "n9kLCPT34rpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# EXTRAIRE LES VALEURS DIRECTEMENT DES OBJETS\n",
        "nb_offres = len(texte_corps)\n",
        "nb_mots_total = len(caract_b_rsw)\n",
        "nb_lemmes_uniques = pd.DataFrame(caract_b_rsw, columns=[\"mot\",\"lemme\",\"pos\"])['lemme'].nunique()\n",
        "\n",
        "# Répartition POS\n",
        "df_mots = pd.DataFrame(caract_b_rsw, columns=[\"mot\",\"lemme\",\"pos\"])\n",
        "pos_counts = df_mots['pos'].value_counts()\n",
        "\n",
        "# Limiter aux stats PEMIERES\n",
        "stats = {\n",
        "    'Nombre d’offres': nb_offres,\n",
        "    'Nombre total de mots': nb_mots_total,\n",
        "    'Nombre de lemmes uniques': nb_lemmes_uniques,\n",
        "    'Nombre de NOUN': pos_counts.get('NOUN', 0),\n",
        "    'Nombre de VERB': pos_counts.get('VERB', 0),\n",
        "    'Nombre de ADJ': pos_counts.get('ADJ', 0),\n",
        "    'Nombre de PROPN': pos_counts.get('PROPN', 0),\n",
        "    'Nombre de PUNCT': pos_counts.get('PUNCT', 0),\n",
        "    'Nombre de X': pos_counts.get('X', 0)\n",
        "}\n",
        "\n",
        "# Créer DataFrame\n",
        "df_table = pd.DataFrame(list(stats.items()), columns=['Statistiques', 'Valeur'])\n",
        "\n",
        "# --- Générer image du tableau ---\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "table = ax.table(cellText=df_table.values,\n",
        "                 colLabels=df_table.columns,\n",
        "                 cellLoc='center',\n",
        "                 colColours=['#1f77b4', '#1f77b4'],\n",
        "                 colWidths=[0.6, 0.3],\n",
        "                 loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "table.scale(1, 2)\n",
        "table_props = table.properties()['children']\n",
        "\n",
        "# Colorer les cellules (bleu clair pour le corps)\n",
        "for cell in table_props[1:]:\n",
        "    cell.set_facecolor('#e6f2ff')\n",
        "\n",
        "plt.title(\"Résumé des statistiques textuelles des offres\", fontsize=14, color='#1f77b4', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6RV-zKjm4uqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOP 5 VERBES NOMS  & ADJECTIFS"
      ],
      "metadata": {
        "id": "jtc6TE898GtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# On prend le top 5 de chaque catégorie\n",
        "top_verbs_5 = top_verbs[:5]\n",
        "top_adjs_5  = top_adjs[:5]\n",
        "top_nouns_5 = top_nouns[:5]\n",
        "\n",
        "# Créer les DataFrames avec catégorie\n",
        "df_verbs = pd.DataFrame(top_verbs_5, columns=['Mot', 'Occurrences'])\n",
        "df_verbs['Catégorie'] = 'Verbe'\n",
        "\n",
        "df_adjs = pd.DataFrame(top_adjs_5, columns=['Mot', 'Occurrences'])\n",
        "df_adjs['Catégorie'] = 'Adjectif'\n",
        "\n",
        "df_nouns = pd.DataFrame(top_nouns_5, columns=['Mot', 'Occurrences'])\n",
        "df_nouns['Catégorie'] = 'Nom'\n",
        "\n",
        "# Fusionner les trois DataFrames\n",
        "df_top5 = pd.concat([df_verbs, df_adjs, df_nouns], ignore_index=True)\n",
        "df_top5 = df_top5[['Mot', 'Catégorie', 'Occurrences']]\n",
        "\n",
        "# Création de la figure pour le tableau\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "ax.axis('off')  # on cache les axes\n",
        "\n",
        "# Ajouter le tableau\n",
        "tbl = ax.table(cellText=df_top5.values,\n",
        "               colLabels=df_top5.columns,\n",
        "               cellLoc='center',\n",
        "               loc='center')\n",
        "\n",
        "tbl.auto_set_font_size(False)\n",
        "tbl.set_fontsize(12)\n",
        "tbl.scale(1, 2)  # ajuste la hauteur des lignes\n",
        "\n",
        "# Sauvegarde en image\n",
        "plt.savefig(\"tableau_top5.png\", bbox_inches='tight', dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nw_V99wW78_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAPHIQUE PAR CATEGORIE DE HARDSKILLS ET PYTHON MODULES"
      ],
      "metadata": {
        "id": "dRq8r809byLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "# 9/ LISTE HARD SKILLS\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# --- Liste initiale et additionnelle de hard skills ---\n",
        "hard_skills = [\"Python\", \"R\", \"SQL\", \"NoSQL\", \"MongoDB\", \"PostgreSQL\", \"MySQL\", \"Oracle\",\n",
        "               \"Excel\", \"VBA\", \"Tableau\", \"PowerBI\", \"QlikView\", \"Looker\", \"Google Data Studio\",\n",
        "               \"SAS\", \"SPSS\", \"Hadoop\", \"Spark\", \"PySpark\", \"TensorFlow\", \"Keras\", \"Scikit-learn\",\n",
        "               \"PyTorch\", \"Numpy\", \"Pandas\", \"Matplotlib\", \"Seaborn\", \"Plotly\", \"Dash\", \"D3.js\",\n",
        "               \"API\", \"Cloud\", \"Git\", \"GitHub\", \"GitLab\", \"Docker\", \"Kubernetes\", \"Airflow\",\n",
        "               \"Jenkins\", \"ETL\", \"Talend\", \"Informatica\", \"Data Warehousing\", \"Data Modeling\",\n",
        "               \"BigQuery\", \"Redshift\", \"Snowflake\", \"Azure\", \"AWS\", \"GCP\", \"Machine Learning\",\n",
        "               \"Deep Learning\", \"NLP\", \"Computer Vision\", \"Streamlit\", \"Power Query\", \"Power Pivot\",\n",
        "               \"Tableau Prep\", \"dbt\", \"Jupyter\", \"Colab\", \"RStudio\", \"KNIME\", \"Alteryx\", \"Excel Modeling\",\n",
        "               \"HBase\", \"Cassandra\", \"Kafka\", \"Flink\", \"NiFi\", \"ElasticSearch\", \"Databricks\", \"Hive\",\n",
        "               \"Pig\", \"Spark SQL\", \"Spark Streaming\", \"MLlib\", \"OpenCV\", \"NLTK\", \"spaCy\", \"Gensim\",\n",
        "               \"FastAPI\", \"Flask\", \"Django\", \"Neo4j\", \"Redis\", \"RabbitMQ\", \"Airbyte\", \"Prefect\", \"Luigi\"]\n",
        "\n",
        "nouveaux_terms = [\n",
        "    \"python\", \"sql\", \"excel\", \"power bi\", \"tableau\", \"sas\", \"r\", \"java\", \"spark\",\n",
        "    \"hadoop\", \"airflow\", \"git\", \"linux\", \"machine learning\", \"deep learning\",\n",
        "    \"data visualization\", \"etl\", \"cloud\", \"azure\", \"aws\", \"gcp\", \"nosql\",\n",
        "    \"pandas\", \"numpy\", \"scikit-learn\", \"tensorflow\", \"pytorch\", \"kafka\", \"powerpoint\",\n",
        "    \"analyse de données\", \"statistiques\", \"data engineering\", \"big data\"\n",
        "]\n",
        "\n",
        "# --- Fusion et nettoyage (tout en minuscules + suppression des doublons) ---\n",
        "hard_skills_fusion = sorted(set([h.lower() for h in hard_skills] + [n.lower() for n in nouveaux_terms]))\n",
        "print(f\"Nombre total de hard skills fusionnées : {len(hard_skills_fusion)}\\n\")\n",
        "\n",
        "# --- Lecture et nettoyage du texte ---\n",
        "with open(\"Emploidata.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texte = f.read().lower()\n",
        "\n",
        "# --- Comptage des occurrences exactes ---\n",
        "competences_trouvees = []\n",
        "for comp in hard_skills_fusion:\n",
        "    occur = len(re.findall(rf\"\\b{re.escape(comp)}\\b\", texte))\n",
        "    if occur > 0:\n",
        "        competences_trouvees.append((comp, occur))\n",
        "\n",
        "# --- Préparer le dictionnaire pour le WordCloud ---\n",
        "competences_dict = dict(competences_trouvees)\n",
        "\n",
        "# 10--- Génération du nuage ---\n",
        "plt.figure(figsize=(10,6))\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color=\"white\",\n",
        "                      colormap=\"viridis\", max_words=50).generate_from_frequencies(competences_dict)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Compétences techniques les plus citées dans les offres de stage (WordCloud)\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# --- Affichage top 15 compétences ---\n",
        "df_comp = pd.DataFrame(competences_trouvees, columns=[\"Compétence\", \"Occurrences\"]).sort_values(by=\"Occurrences\", ascending=False)\n",
        "print(\"\\nTop 15 compétences citées :\")\n",
        "print(df_comp.head(15))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2UJVWhO2QOaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "N GRAMMES"
      ],
      "metadata": {
        "id": "NLviEQ6nJxwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk import bigrams, trigrams\n",
        "\n",
        "# --- Mots à exclure ---\n",
        "exclus = {\n",
        "    'fin','étude','poste','description','offre','propos',\n",
        "    'principal','mission','profil','compétence','étudiant'\n",
        "}\n",
        "exclus_tri = exclus.union({'contrat','stage','type'})\n",
        "\n",
        "# --- Nettoyage des lemmes ---\n",
        "lemmes_clean = [\n",
        "    lemme for (mot, lemme, pos) in caract_b_rsw\n",
        "    if lemme.isalpha() and lemme not in exclus\n",
        "]\n",
        "\n",
        "# --- Bigrammes filtrés ---\n",
        "bi = [\n",
        "    b for b in bigrams(lemmes_clean)\n",
        "    if b not in [(\"mise\",\"place\"), (\"mise\",\"œuvre\"), (\"mise\",\"oeuvre\")]\n",
        "]\n",
        "top_bi = Counter(bi).most_common(10)\n",
        "\n",
        "# --- Trigrammes filtrés ---\n",
        "tri = [\n",
        "    t for t in trigrams(lemmes_clean)\n",
        "    if not any(w in exclus_tri for w in t)\n",
        "    and (\"mise\",\"place\") not in (t[:2], t[1:])\n",
        "    and (\"mise\",\"œuvre\") not in (t[:2], t[1:])\n",
        "]\n",
        "top_tri = Counter(tri).most_common(10)\n",
        "\n",
        "# --- DataFrames pour affichage texte ---\n",
        "df_bigrams = pd.DataFrame(top_bi, columns=['Bigramme', 'Occurrences'])\n",
        "df_bigrams['Bigramme'] = df_bigrams['Bigramme'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "df_trigrams = pd.DataFrame(top_tri, columns=['Trigramme', 'Occurrences'])\n",
        "df_trigrams['Trigramme'] = df_trigrams['Trigramme'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "print(\"Top 10 Bigrammes :\")\n",
        "print(df_bigrams)\n",
        "\n",
        "print(\"\\nTop 10 Trigrammes :\")\n",
        "print(df_trigrams)\n",
        "\n",
        "# --- Visualisation ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Bigrammes\n",
        "axes[0].barh(df_bigrams['Bigramme'], df_bigrams['Occurrences'], color='steelblue')\n",
        "axes[0].set_title(\"Top 10 Bigrammes\")\n",
        "axes[0].set_xlabel(\"Fréquence\")\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Trigrammes\n",
        "axes[1].barh(df_trigrams['Trigramme'], df_trigrams['Occurrences'], color='darkorange')\n",
        "axes[1].set_title(\"Top 10 Trigrammes\")\n",
        "axes[1].set_xlabel(\"Fréquence\")\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aZkwfZoTeART"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "soft skills\n"
      ],
      "metadata": {
        "id": "fd3uiMQwkaO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Méga-liste de soft skills\n",
        "soft_skills = [\n",
        "    \"rigoureux\", \"rigoureuse\", \"autonome\", \"curieux\", \"curieuse\", \"réactif\", \"réactive\",\n",
        "    \"organisé\", \"organisée\", \"méthodique\", \"créatif\", \"créative\", \"esprit d'équipe\",\n",
        "    \"communication\", \"collaboratif\", \"collaborative\", \"flexible\", \"proactif\", \"proactive\",\n",
        "    \"analytique\", \"responsable\", \"motivé\", \"motivée\", \"adaptable\", \"persévérant\",\n",
        "    \"persévérante\", \"esprit critique\", \"sociable\", \"dynamique\", \"autodidacte\",\n",
        "    \"curiosité intellectuelle\", \"initiative\", \"esprit d'analyse\", \"esprit de synthèse\",\n",
        "    \"sens de l'organisation\", \"gestion du temps\", \"capacité à travailler sous pression\",\n",
        "    \"capacité d'adaptation\", \"rigueur\", \"collaboration\", \"coopératif\", \"coopérative\",\n",
        "    \"esprit d'initiative\", \"leadership\", \"capacité à résoudre des problèmes\", \"créativité\",\n",
        "    \"sens de l'écoute\", \"esprit logique\", \"capacité à prioriser\", \"polyvalent\", \"polyvalente\",\n",
        "    \"motivation\", \"engagement\", \"curiosité\", \"persistance\", \"autonomie\", \"organisation\",\n",
        "    \"capacité d'apprentissage rapide\", \"qualité relationnelle\", \"sens de la communication\",\n",
        "    \"orientation résultats\", \"capacité d'analyse\", \"travail en équipe\", \"gestion des conflits\",\n",
        "    \"esprit stratégique\", \"capacité à travailler en autonomie\", \"capacité à synthétiser\",\n",
        "    \"capacité de décision\", \"force de proposition\", \"esprit positif\", \"relationnel\"\n",
        "]\n",
        "\n",
        "# Lecture du texte\n",
        "with open(\"Emploidata.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texte = f.read().lower()\n",
        "\n",
        "# Comptage des occurrences\n",
        "competences_soft = []\n",
        "for skill in soft_skills:\n",
        "    occur = len(re.findall(rf\"\\b{re.escape(skill.lower())}\\b\", texte))\n",
        "    if occur > 0:\n",
        "        competences_soft.append((skill, occur))\n",
        "\n",
        "# DataFrame trié par fréquence\n",
        "df_soft = pd.DataFrame(competences_soft, columns=[\"Qualité\", \"Occurrences\"]).sort_values(by=\"Occurrences\", ascending=False)\n",
        "\n",
        "# Top 5\n",
        "top_soft = df_soft.head(5)\n",
        "print(\"Top 5 Soft Skills / Qualités valorisées :\")\n",
        "print(top_soft)\n",
        "\n",
        "# Dictionnaire pour WordCloud\n",
        "soft_dict = dict(zip(df_soft['Qualité'], df_soft['Occurrences']))\n",
        "wordcloud = WordCloud(width=600, height=400, background_color=\"white\",\n",
        "                      colormap=\"cool\", max_words=50).generate_from_frequencies(soft_dict)\n",
        "\n",
        "# Affichage combiné\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14,6))\n",
        "\n",
        "# Graphique en barres\n",
        "axs[0].barh(top_soft['Qualité'][::-1], top_soft['Occurrences'][::-1], color='skyblue')\n",
        "axs[0].set_xlabel(\"Occurrences\")\n",
        "axs[0].set_title(\"Top 5 Soft Skills / Qualités valorisées\")\n",
        "\n",
        "# WordCloud\n",
        "axs[1].imshow(wordcloud, interpolation=\"bilinear\")\n",
        "axs[1].axis(\"off\")\n",
        "axs[1].set_title(\"WordCloud - Soft Skills valorisées\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dLQ9fmSJ9lXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTEURS QUI RECRUTENT LE PLUS"
      ],
      "metadata": {
        "id": "gYwkfOIK8oHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import re\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Lecture du corpus ---\n",
        "with open(\"Emploidata.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texte = f.read()\n",
        "\n",
        "# --- Découpage des offres\n",
        "offres = [o.strip() for o in texte.split(\"****\") if o.strip()]\n",
        "\n",
        "# --- Dictionnaire de secteurs par mots-clés\n",
        "secteurs_keywords = {\n",
        "    r\"\\b(banque|finance|assurance|crédit)\\b\": \"Banque & Assurance\",\n",
        "    r\"\\b(santé|pharma|médical|hôpital|clinique|mutuelle)\\b\": \"Santé\",\n",
        "    r\"\\b(énergie|edf|total|engie|gaz|nucléaire)\\b\": \"Énergie\",\n",
        "    r\"\\b(industrie|aéronautique|automobile|bâtiment|vinci|production)\\b\": \"Industrie\",\n",
        "    r\"\\b(retail|commerce|distribution|grande surface|carrefour|auchan|decathlon|lagardère)\\b\": \"Retail / Commerce\",\n",
        "    r\"\\b(conseil|consultant|cabinet|equancy|capgemini|ey|pwc|kpmg|deloitte)\\b\": \"Conseil & Services\",\n",
        "    r\"\\b(technologie|informatique|digital|data|ia|intelligence artificielle|it|logiciel|start-up)\\b\": \"Tech / Informatique\",\n",
        "    r\"\\b(enseignement|université|école|institut|éducation)\\b\": \"Éducation\",\n",
        "    r\"\\b(public|ministère|administration|mairie|collectivité)\\b\": \"Secteur public\",\n",
        "}\n",
        "\n",
        "# --- Extraction des entreprises et secteurs\n",
        "entreprises = []\n",
        "secteurs = []\n",
        "\n",
        "for offre in offres:\n",
        "    lignes = [l.strip() for l in offre.splitlines() if l.strip()]\n",
        "    entreprise = lignes[0] if lignes else \"Inconnue\"\n",
        "    entreprises.append(entreprise)\n",
        "\n",
        "    secteur_trouve = \"Autre\"\n",
        "    for motif, secteur in secteurs_keywords.items():\n",
        "        if re.search(motif, offre.lower()):\n",
        "            secteur_trouve = secteur\n",
        "            break\n",
        "    secteurs.append(secteur_trouve)\n",
        "\n",
        "# --- Création du DataFrame\n",
        "df_secteurs = pd.DataFrame({\"Entreprise\": entreprises, \"Secteur\": secteurs})\n",
        "\n",
        "# --- Comptage des occurrences\n",
        "top_secteurs = df_secteurs[\"Secteur\"].value_counts().head(10).reset_index()\n",
        "top_secteurs.columns = [\"Secteur\", \"Nombre d'offres\"]\n",
        "\n",
        "# --- Visualisation\n",
        "sns.set(style=\"whitegrid\", palette=\"Set2\")\n",
        "plt.figure(figsize=(9,6))\n",
        "sns.barplot(data=top_secteurs, x=\"Nombre d'offres\", y=\"Secteur\")\n",
        "plt.title(\"Top 10 secteurs les plus recruteurs dans les offres de stage\", fontsize=14)\n",
        "plt.xlabel(\"Nombre d'occurrences\")\n",
        "plt.ylabel(\"Secteur d'activité\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Affichage texte\n",
        "print(\"\\nTop 10 secteurs les plus recruteurs :\")\n",
        "print(top_secteurs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cn9l6Los8tjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROFILS LES PLUS RECHERCHES"
      ],
      "metadata": {
        "id": "rxzMDpun_VH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import re\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialisation des listes pour stocker les intitulés et les entreprises\n",
        "intitules = []\n",
        "entreprises = []\n",
        "\n",
        "# Parcours de chaque offre pour extraire les informations utiles\n",
        "for offre in offres:\n",
        "    lignes = [l.strip() for l in offre.splitlines() if l.strip()]\n",
        "    entreprise = lignes[0] if lignes else \"Inconnue\"\n",
        "    entreprises.append(entreprise)\n",
        "\n",
        "    # Recherche d'expressions liées aux métiers de la data\n",
        "    match = re.search(\n",
        "        r\"(data\\s+(analyst|scientist|engineer|consultant|manager|specialist))|\"\n",
        "        r\"(business\\s+analyst)|\"\n",
        "        r\"(chef\\s+de\\s+projet\\s+(data|digital|bi))|\"\n",
        "        r\"(charg[eé]\\s+d['e]\\s+(études|analyse|statistiques?))|\"\n",
        "        r\"(ingénieur\\s+(data|ia|intelligence\\s+artificielle|informatique))|\"\n",
        "        r\"(consultant\\s+(data|bi|digital|ia))|\"\n",
        "        r\"(alternant|stagiaire|assistant)\\s+(en\\s+(data|analyse|bi|statistique|informatique))\",\n",
        "        offre.lower()\n",
        "    )\n",
        "\n",
        "    titre = match.group(0).strip().title() if match else \"Autre\"\n",
        "    intitules.append(titre)\n",
        "\n",
        "# Création du tableau avec les intitulés bruts\n",
        "df_jobs = pd.DataFrame({\"Entreprise\": entreprises, \"Intitulé brut\": intitules})\n",
        "\n",
        "# Fonction pour regrouper les intitulés similaires sous un même nom\n",
        "def normaliser_titre(t):\n",
        "    t = t.lower().strip()\n",
        "    t = re.sub(r\"h/f|alternance|stagiaire|stage|assistant|junior|senior\", \"\", t)\n",
        "    t = t.replace(\"  \", \" \").strip()\n",
        "    if \"business analyst\" in t:\n",
        "        return \"Business Analyst\"\n",
        "    elif \"data analyst\" in t:\n",
        "        return \"Data Analyst\"\n",
        "    elif \"data scientist\" in t:\n",
        "        return \"Data Scientist\"\n",
        "    elif \"data engineer\" in t:\n",
        "        return \"Data Engineer\"\n",
        "    elif \"consultant\" in t:\n",
        "        return \"Consultant Data/BI\"\n",
        "    elif \"chef de projet\" in t or \"project manager\" in t:\n",
        "        return \"Chef de projet Data/Digital\"\n",
        "    elif \"charg\" in t or \"étude\" in t or \"statist\" in t:\n",
        "        return \"Chargé d'études statistiques\"\n",
        "    elif \"intelligence artificielle\" in t or \"ia\" in t or \"ml\" in t:\n",
        "        return \"Ingénieur IA\"\n",
        "    elif \"data manager\" in t:\n",
        "        return \"Data Manager\"\n",
        "    elif \"bi analyst\" in t:\n",
        "        return \"BI Analyst\"\n",
        "    elif \"data architect\" in t:\n",
        "        return \"Data Architect\"\n",
        "    elif \"data quality\" in t or \"qualité données\" in t:\n",
        "        return \"Data Quality Analyst\"\n",
        "    else:\n",
        "        return \"Autre\"\n",
        "\n",
        "# Application de la normalisation\n",
        "df_jobs[\"Intitulé normalisé\"] = df_jobs[\"Intitulé brut\"].apply(normaliser_titre)\n",
        "\n",
        "# On garde uniquement les intitulés reconnus\n",
        "df_jobs = df_jobs[df_jobs[\"Intitulé normalisé\"] != \"Autre\"]\n",
        "\n",
        "# Comptage des métiers les plus fréquents\n",
        "top_jobs = df_jobs[\"Intitulé normalisé\"].value_counts().head(10).reset_index()\n",
        "top_jobs.columns = [\"Métier\", \"Nombre d'offres\"]\n",
        "\n",
        "# Affichage du graphique\n",
        "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
        "plt.figure(figsize=(9,6))\n",
        "sns.barplot(data=top_jobs, x=\"Nombre d'offres\", y=\"Métier\")\n",
        "plt.title(\"Top 10 métiers les plus demandés\", fontsize=14)\n",
        "plt.xlabel(\"Nombre d'occurrences\")\n",
        "plt.ylabel(\"Intitulé du poste\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Impression du tableau\n",
        "print(top_jobs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3v8p1EVq_ZfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLUSTERING/KMEANS"
      ],
      "metadata": {
        "id": "cEpxvB8gbUMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Créer  une liste de “documents” où chaque document = une offre\n",
        "documents = []\n",
        "\n",
        "for offre in texte_corps:\n",
        "    doc = nlp(offre)\n",
        "    mots_filtrés = []\n",
        "    for sentence in doc.sentences:\n",
        "        for word in sentence.words:\n",
        "            lemme = word.lemma.lower()\n",
        "            pos = word.upos\n",
        "            if (lemme not in stop_words_liste\n",
        "                and not re.search(r'\\d', lemme)\n",
        "                and ((pos == 'NOUN' and lemme not in noms_generiques)\n",
        "                     or (pos == 'VERB' and lemme not in auxiliaires)\n",
        "                     or (pos == 'ADJ' and lemme not in adjs_generiques))):\n",
        "                mots_filtrés.append(lemme)\n",
        "    documents.append(\" \".join(mots_filtrés))\n",
        "\n",
        "print(f\"Nombre de documents pour clustering : {len(documents)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Dkgt8fd6UWAt",
        "outputId": "51b6d516-1080-40fb-8a81-19ed85b3af9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de documents pour clustering : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#TF-IDF Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "terms = vectorizer.get_feature_names_out()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xWZTYVIAXDYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Méthode du coude ---\n",
        "K = range(1, 10)\n",
        "inerties = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k, random_state=42).fit(X)\n",
        "    inerties.append(km.inertia_)\n",
        "\n",
        "# --- 2. Clustering avec k=2 ---\n",
        "k = 2\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "cluster_counts = pd.Series(labels).value_counts().sort_index()\n",
        "\n",
        "# --- 3. Affichage : coude + répartition côte à côte ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# A gauche : inertie (méthode du coude)\n",
        "axes[0].plot(K, inerties, marker='o', color='steelblue')\n",
        "axes[0].set_title(\"Méthode du coude (Inertie)\")\n",
        "axes[0].set_xlabel(\"Nombre de clusters (k)\")\n",
        "axes[0].set_ylabel(\"Inertie\")\n",
        "axes[0].grid(True, linestyle=\"--\", alpha=0.6)\n",
        "axes[0].axvline(2, color='red', linestyle='--', label='k = 2')\n",
        "axes[0].legend()\n",
        "\n",
        "# A droite : répartition\n",
        "axes[1].bar(cluster_counts.index, cluster_counts.values, color='skyblue')\n",
        "axes[1].set_title(f\"Répartition des {cluster_counts.sum()} offres (k=2)\")\n",
        "axes[1].set_xlabel(\"Cluster\")\n",
        "axes[1].set_ylabel(\"Nombre d'offres\")\n",
        "for i, v in enumerate(cluster_counts.values):\n",
        "    axes[1].text(i, v + 1, str(v), ha='center', fontsize=11, fontweight='bold')\n",
        "axes[1].grid(axis='y', linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 4. Résumé numérique ---\n",
        "print(\"Répartition des offres par cluster :\")\n",
        "print(cluster_counts)\n",
        "\n",
        "# --- 5. Tableau lexical (mots caractéristiques de chaque cluster) ---\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "cluster_lexical = {}\n",
        "\n",
        "for i in range(k):\n",
        "    indices = np.where(labels == i)[0]\n",
        "    cluster_matrix = X[indices].mean(axis=0)\n",
        "    top_indices = np.array(cluster_matrix).argsort()[0][-15:][::-1]\n",
        "    cluster_lexical[f\"Cluster {i}\"] = [terms[j] for j in top_indices]\n",
        "\n",
        "lexical_df = pd.DataFrame.from_dict(cluster_lexical, orient='index', columns=[f\"Mot {i+1}\" for i in range(15)])\n",
        "print(\"Tableau lexical par cluster :\")\n",
        "print(lexical_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rh2ArHmDXgax"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}